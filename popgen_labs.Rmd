---
title: "Potential Population Genetics Labs & Content"
output: pdf_document
bibliography: /home/antolinlab/Desktop/library.bib
---

<!--- bibliography: /users/kelly/desktop/library.bib -->

<!--- Set text wrapping for embedded R code -->
```{r set-options, echo=FALSE, }
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60))
```

<!--- Most of the old stuff was a bit beyond the scope of the course, so I've streamlined it a bit -->
# Next-generation sequencing (NGS)

Next-generation sequencing commonly refers to a set of DNA sequencing platforms that perform *short-read* sequencing. Short-read sequencing refers to the limit on the number of bases that can be read; typically less than 250 bases can be read in a single contiguous sequence. Users specify the exact number of bases they would like read, in increments of 25 bases, up to the 250 base limit (e.g., 50, 75, 100 bases, etc.) Traditional sequencing methods can sequence much longer contiguous stretches of DNA. The benefit of next-generation sequencing is that these platforms are *high-throughput*: they allow many samples to be sequenced in a *multiplexed* fashion where DNA from many individuals can be uniquely *barcoded* and mixed together. This produces high volumes of sequence data. NGS platforms can be used to sequence full genomes, or to sequence smaller subsets of the genome at a lower cost. Our lab exercises will focus on *reduced-representation* NGS, where genomic DNA is broken up either by restriction enzymes or sonication, and a small subset of the resulting fragments are sequenced.

The increased efficiency of NGS comes at a cost. In addition to the short-read constraint, NGS is a very error-prone technology. The expected probability that a nucleotide base will be incorrectly read is 0.1% -- while this is a small number, when hundreds of millions of bases are read, the result is a number of mis-identified bases. This measurement error is compensated for by re-reading each piece of DNA multiple times. The number of times the same piece of DNA is sequenced is called the *depth of coverage* for that sequence. Each base that is read by the sequencing instrument is also assigned a quality score that helps in determining the true sequence.

Inferring true sequence identity from the sequence data delivered by the sequencing instrument is by itself a rich area of research. Bioinformaticians develop algorithms to handle the massive quantities of data generated by NGS projects, identify sequencing errors using depth of coverage and quality scores, and ultimately determine the sequence of DNA to a high degree of statistical certainty. This work typically requires a supercomputer, but small datasets can be handled by personal computers. You will be working with data that has already been processed through the appropriate bioinformatic *pipeline* to remove sequence errors and *call genotypes*. 

Processing NGS data through these bioinformatic pipelines is beyond the scope of this course, but it is worth understanding (briefly) what it takes to go from a DNA sample to the sequence data needed to answer population-genetic questions.

## 1. Demultiplexing & quality filtering raw data

- Check that the length of each read matches the expectation.
- If DNA was prepared for sequencing using restriction digestion, check that the enzyme overhangs expected are present in the sequence.
- Separate sequences from different individuals using their unique sample barcodes.

## 2. Assembly
### 2a. Reference-based assembly

    - If the samples you have sequenced are from an organism with a fully sequenced genome, such as humans or yeast, then the reference genome sequence is very useful in determining which new DNA sequences are correct (error-free) and from which part of the genome they were sequenced. 

# Glossary
|Term|Definiton|
|:-------|:------------|
|next-generation sequencing (NGS)||
|short-read||
|high-throughput||
|multiplexed||
|sequence-by-synthesis||
|depth of coverage||
|pipeline||
|genotype calling||
|quality score||
|barcode||

Brief overview and list of platforms.
Some discussion of error rates & necessary coverage depths.

## Uses & Common Techniques

- Whole genome sequencing & re-sequencing
- Biodiversity studies (e.g., gut microbiome or soil microbial diversity)
- Reduced representation genome sequencing
    - RADseq
    - 2b-RAD
    - ddRADseq

# NGS Data Processing

This will be an overview of the general workflow for handling NGS data derived from reduced-representation and whole genome resequencing projects. Links to online resources and tutorials will be included.

## Demultiplexing

Raw NGS data files are very large and contain the sequences for all individuals in your sample mixed together. The unique sample adapters ligated to the restriction fragments to sequenc

## Reference-based assembly

## *de novo* Assembly

## Quality Filters
**genotype quality**

- guideline: remove SNPs with quality scores <25

- rationale: remove low quality genotypes from analysis
  
**depth of coverage**

- guideline: retain sequences only with >=10X

- rationale: include only high-quality genotypes observed several times in the same individal for increased confidence in genotype calls
  
**minor allele frequency**

- guideline: remove <5% frequency

- rationale: remove private alleles and sequencing errors not caught in other filters, at the risk of also removing informative information about rare alleles

\pagebreak

# Overview of data formats and maniplation of VCF data in R

One of the most challenging aspects of analyzing NGS data is converting data to the different formats required for input into analysis software.

Here is a brief summary of the different data types types used by common R packages for population genetic inference.

|Package|Input Format|Link|
|:-------:|:------------:|:-------------------|
|adegenet|genlight |http://adegenet.r-forge.r-project.org/|
|genetics|genotype file|https://cran.r-project.org/web/packages/genetics/index.html|
|SNPrelate|GDS|http://www.bioconductor.org/packages/release/bioc/html/SNPRelate.html|
|poppr|geneclone|http://grunwaldlab.cgrb.oregonstate.edu/poppr-r-package-population-genetics|
|GWAStools|NetCDF, GDS or Matrix|http://www.bioconductor.org/packages/release/bioc/html/GWASTools.html|

For a more detailed discussion of different data types and their pros/cons, please see https://github.com/NESCent/r-popgen-hackathon/wiki/R-Classes-for-Population-Genetic-Data .

\pagebreak

# Exercise 1: Structure at the locus-level: Calculation of $F_{ST}$

## Background

Genetic variation in populations can exist within subpopulations ("groups") and between groups. We often ask how divergent subpopulations are by considering the partitioning of genetic variation into these categories. If most of the population genetic variation is partitioned **between** groups, we infer that these groups are genetically diverged in some manner. We would then evaluate possible hypotheses for this divergence. 

Alternatively, if most of the population genetic variation is partitioned **within** groups, we infer that the groups are not highly diverged. In this case, even if there is a high amount of genetic variation in the poplation as a whole, there is no difference in the genetic variation contained within each group.

We can measure genetic variation in subpopulations in a number of ways. The classical method is to calculate the Fixation Index, $F_{ST}$ [@Brown1970]. High $F_{ST}$ values indicate genetic variation is partitioned **between** groups and suggests the presence of meaningful population structure.

Evolutionary rates vary throughout the genome, and consequently $F_{ST}$ can vary for different loci under consideration. Just as some loci are highly conserved while others are highly divergent, $F_{ST}$ can be very high for some loci and very low for others. Next-generation sequencing produces data for numerous loci, and we can calculate $F_{ST}$ for each locus. This provides us with a distribution of $F_{ST}$ values across the genome a nd allows us to identify sites that correspond with population structure.

### Exercise options

1. Find a reference mapped dataset and calculate sliding window $F_{ST}$
2. Take a small subset of the tick dataset and do an $F_{ST}$ outlier analysis using Bayescan (would need to be installed in computer lab or on a remote machine to which students can submit computing jobs).
3. Take already completed Bayescan output, visualize, and interpret


### _Rough outline for option 1_
<!---working through the tutorial found here: https://cran.r-project.org/web/packages/PopGenome/vignettes/An_introduction_to_the_PopGenome_package.pdf -->

```{r, echo=FALSE, eval=FALSE}
# for Windows/OSX
install.packages("PopGenome")
# for Ubuntu
install.packages("~/Downloads/PopGenome_2.1.6.tar.gz", repos=NULL, type="source")
install.packages("~/Downloads/WhopGenome_0.9.2.tar.gz", repos=NULL, type="source")

# read in the VCF file as a regular table to get some quick info...
tick.table<-read.table('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED.vcf', sep="\t", header=T, comment.char="")

# column 2 is the position
min(tick.table[,2])
max(tick.table[,2])

# pretend we have a single chromosome represented in this VCF file... how many bases would there be?
200*length(tick.table[,1])
# 477,400 is a bit long for a chromosome

# make a fake 100,000 base chrom
tick.subset<-tick.table[1:(100000/200),]

# update the position info to make all the position points unique
# each row represents a SNP found on a 200bp long concatenated segment of the reference
# some 200bp stretches have >1 SNP, but since we're making stuff up anyway we won't really worry about that
for(row in 1:length(tick.table[,1])){
  tick.table[row,2]=tick.table[row,2]+(row*200)
}

pop1<-names(tick.table)[10:92]
pop1.fix<-sub('.', '-', pop1, fixed=T)
pop2<-names(tick.table)[93:108]
pop2.fix<-sub('.', '-', pop2, fixed=T)

write.table(pop1.fix, file='~/Desktop/CSU_PopGen_Labs/pop1.txt', quote=F, row.names=F, col.names=F)
write.table(pop2.fix, file='~/Desktop/CSU_PopGen_Labs/pop2.txt', quote=F, row.names=F, col.names=F)

head(tick.subset)[,1:5]
tail(tick.subset)[,1:5]

# make it appear as though all those snps are on the same chromosome
tick.table[,1]<-rep('1', length(tick.table[,1]))

# fix the names
names(tick.table)[1]<-"#CHROM"

write.table(tick.table, '~/Desktop/CSU_PopGen_Labs/chrom1_tick_dummy_data.vcf', quote=FALSE, row.names=FALSE, sep="\t")

min(tick.subset$POS)
max(tick.table$POS)

# tick dummy data isn't great... what about human data from NCBI?

human.table<-read.table('~/Downloads//00-common_all.vcf.gz')

# utility just for reading VCF files into R
source("http://bioconductor.org/biocLite.R")
biocLite("VariantAnnotation")
biocLite("snpStats")
library(VariantAnnotation)
fl<-system.file('extdata', 'chr7-sub.vcf.gz', package='VariantAnnotation')
vcf1<-readVcf(fl, 'hg19')
vcf1

snpmat<-genotypeToSnpMatrix(vcf1)

library(ade4)
data(humDNAm)
attributes(humDNAm)
amovahum<-amova(humDNAm$samples)
amovahum

#http://grunwaldlab.github.io/Population_Genetics_in_R/AMOVA.html
```

<!--- some command line things to index the newly created VCF file

# retrieve the header for the VCF file... opening it partially in gedit allows us to scroll down and see the first 417,187 lines are header. put those in a new file. (we're using data I processed through my merge_vcf.r script, which removes the header)
$ cd ~/Desktop/D_variabilis_Pseudoref/MasterPseudoRefVCF_Copy/
$ head -417187 pseudoref_mapped_genotypes.vcf >> pseudoref_vcf_header.txt
$ cp pseudoref_vcf_header.txt ~/Desktop/CSU_PopGen_Labs/

# paste the header onto the filtered data file we're using
$ cd ~/Desktop/CSU_PopGen_Labs/
$ cat pseudoref_vcf_header.txt chrom1_tick_dummy_data.vcf >> full_chrom1_tick_dummy_data.vcf

# index the full VCF file
$ sudo apt-get install tabix
$ bgzip full_chrom1_tick_dummy_data.vcf
$ tabix -f full_chrom1_tick_dummy_data.vcf.gz
-->

```{r, echo=FALSE, results='hide', include=FALSE}
library(PopGenome)
#library(WhopGenome)
```

```{r, eval=FALSE, tidy=TRUE}
library(PopGenome)
#library(WhopGenome)
```

```{r, eval=FALSE}
# PopGenome is one of the few R packages that can read VCF files directly.
# We'll use the pseudoreference mapped tick genome data
# to calculate a sliding window FST -- totally inappropriate because this is just a subset of the tick genome...
# But it's a chance to learn how the software works.

# population membership
setwd('~/Desktop/CSU_PopGen_Labs/')

# explore the raw data in R (but we can't do analysis directly on this file)
tick.table<-read.table('chrom1_tick_dummy_data.vcf', sep="\t", header=T, comment.char="")

# get population IDs
pop1<-names(tick.table)[10:92]
pop1.fix<-sub('.', '-', pop1, fixed=T)
pop2<-names(tick.table)[93:108]

populations<-list(pop1, pop2)

GENOME.class<-readVCF('full_chrom1_tick_dummy_data.vcf.gz', numcols=2, tid="1", frompos=338, topos=200044)

GENOME.class@n.sites
GENOME.class@n.biallelic.sites #says there are only 13?! something wrong with numcols argument, I think...

GENOME.class<-set.populations(GENOME.class, populations)

GENOME.class.slide<-sliding.window.transform(GENOME.class,100,100)

GENOME.class.slide<-diversity.stats(GENOME.class.slide)
PopGplot(GENOME.class.slide@nuc.diversity.within)

slide.GENOME.slide<-F_ST.stats(GENOME.class, list(1:83, 84:99))
get.F_ST(GENOME.class.slide)
fst.vals<-GENOME.class.slide@F_ST.stats
GENOME.class@Pi
GENOME.class@region.stats

# none of this works right...

# try with stickleback data...
# vcf file from http://datadryad.org/resource/doi:10.5061/dryad.62hb0/1
# lines are out of order; fix with 
# $ vcf-sort batch_2.vcf.gz >> batch_2_sort.vcf
# then deal with the unfortunate reality that this vcf file has missing GT data as ".",
# and not "./." -- this means that PopGenome can't parse the data
# run a short python script that will fix the problem (thx Joe!)... FILE NAMES ARE HARD-CODED
# $ python GTfix.py
# this inserts new lines after every line; rawr! fix is with this:
# $ grep . batch_2_sort_GTfix.vcf >> batch_2_sort_GTrefix.vcf
# then zip with bgzip and index with tabix (see above html-style comment for the basic structure)

stick<-read.table('batch_2_sort.vcf.gz')
stick.fix<-read.table('batch_2_sort_GTfix.vcf.gz', sep="\t")

groupX<-stick.fix[which(stick.fix[,1]=='groupX'),]

setwd('~/Downloads/or_stickleback/')
setwd('~/Desktop/CSU_PopGen_Labs/')
GENOME.class<-readVCF('batch_2_sort_GTrefix.vcf.gz', tid='groupX', numcols=length(stick.fix[,1]), frompos=min(groupX[,2]), topos=max(groupX[,2]))

GENOME.class<-F_ST.stats(GENOME.class, list(1:250, 251:500))
get.F_ST(GENOME.class)

GENOME.class@n.sites
GENOME.class@n.biallelic.sites 

GENOME.class<-set.populations(GENOME.class, populations)

GENOME.class.slide<-sliding.window.transform(GENOME.class)

GENOME.class.slide<-diversity.stats(GENOME.class.slide)
PopGplot(GENOME.class.slide@nuc.diversity.within)

source("http://bioconductor.org/biocLite.R")
biocLite("VariantAnnotation")
library(VariantAnnotation)

```

