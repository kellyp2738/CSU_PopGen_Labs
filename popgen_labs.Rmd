---
title: "Potential Population Genetics Labs & Content"
output: pdf_document
bibliography: /users/kelly/desktop/library.bib
---

<!--- bibliography: /home/antolinlab/Desktop/library.bib -->

<!--- Set text wrapping for embedded R code -->
```{r set-options, echo=FALSE, }
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60))
```

<!--- Most of the old stuff was a bit beyond the scope of the course, so I've streamlined it a bit -->
# Next-generation sequencing (NGS)

Next-generation sequencing commonly refers to a set of DNA sequencing platforms that perform *short-read* sequencing. Short-read sequencing refers to the limit on the number of bases that can be read; typically less than 250 bases can be read in a single contiguous sequence. Users specify the exact number of bases they would like read, in increments of 25 bases, up to the 250 base limit (e.g., 50, 75, 100 bases, etc.) Traditional sequencing methods can sequence much longer contiguous stretches of DNA. The benefit of next-generation sequencing is that these platforms are *high-throughput*: they allow many samples to be sequenced in a *multiplexed* fashion where DNA from many individuals can be uniquely *barcoded* and mixed together. This produces high volumes of sequence data. NGS platforms can be used to sequence full genomes, or to sequence smaller subsets of the genome at a lower cost. Our lab exercises will focus on *reduced-representation* NGS, where genomic DNA is broken up either by restriction enzymes or sonication, and a small subset of the resulting fragments are sequenced.

The increased efficiency of NGS comes at a cost. In addition to the short-read constraint, NGS is a very error-prone technology. The expected probability that a nucleotide base will be incorrectly read is 0.1% -- while this is a small number, when hundreds of millions of bases are read, the result is a number of mis-identified bases. This measurement error is compensated for by re-reading each piece of DNA multiple times. The number of times the same piece of DNA is sequenced is called the *depth of coverage* for that sequence. Each base that is read by the sequencing instrument is also assigned a quality score that helps in determining the true sequence.

Inferring true sequence identity from the sequence data delivered by the sequencing instrument is by itself a rich area of research. Bioinformaticians develop algorithms to handle the massive quantities of data generated by NGS projects, identify sequencing errors using depth of coverage and quality scores, and ultimately determine the sequence of DNA to a high degree of statistical certainty. This work typically requires a supercomputer, but small datasets can be handled by personal computers. You will be working with data that has already been processed through the appropriate bioinformatic *pipeline* to remove sequence errors and *call genotypes*. 

Processing NGS data through these bioinformatic pipelines is beyond the scope of this course, but it is worth understanding (briefly) what it takes to go from a DNA sample to the sequence data needed to answer population-genetic questions.

## 1. Demultiplexing & quality filtering raw data

- Check that the length of each read matches the expectation.
- If DNA was prepared for sequencing using restriction digestion, check that the enzyme overhangs expected are present in the sequence.
- Separate sequences from different individuals using their unique sample barcodes.

## 2. Assembly
### 2a. Reference-based assembly

    - If the samples you have sequenced are from an organism with a fully sequenced genome, such as humans or yeast, then the reference genome sequence is very useful in determining which new DNA sequences are correct (error-free) and from which part of the genome they were sequenced. 

# Glossary
|Term|Definiton|
|:-------|:------------|
|next-generation sequencing (NGS)||
|short-read||
|high-throughput||
|multiplexed||
|sequence-by-synthesis||
|depth of coverage||
|pipeline||
|genotype calling||
|quality score||
|barcode||

Brief overview and list of platforms.
Some discussion of error rates & necessary coverage depths.

## Uses & Common Techniques

- Whole genome sequencing & re-sequencing
- Biodiversity studies (e.g., gut microbiome or soil microbial diversity)
- Reduced representation genome sequencing
    - RADseq
    - 2b-RAD
    - ddRADseq
- Paired-end or single-end sequencing
    
# Paired-end Illumina NGS Data for Reduced-Representation Sequencing

There are several different NGS platforms, but the NGS data we are working with in this class is from Illumina HiSeq Paired-End reads. These data were generated for a reduced-representation library. This data is provided from sequencing facilities in FASTQ format, as specified below.

## Basic format

**Line 1:**

\@Instrument:RunNumber:FlowcellID:Lane:Tile:x-coord:y-coord Read:IsFiltered:ControlNo:SampleID

Fields in the first line with paying attention to:

"Read" indicates if the sequence is read 1 or read 2 in a paired end run. Read 1 and Read 2 will likely be delivered to you in separate files by your sequencing facility. If not, use this field to separate them.

"Sample ID" will be either a number or an Illumina barcode sequence. This indicates the library and *NOT* the individual sample ID.

For more information, see the Illumina data format specification: http://support.illumina.com/content/dam/illumina-support/help/BaseSpaceHelp_v2/Content/Vault/Informatics/Sequencing_Analysis/BS/swSEQ_mBS_FASTQFiles.htm

**Line 2:**

DNA sequence. Consult with your sequencing facility to see to what extent your sequences have been trimmed. Some facilities remove non-informative data (like Illumina primer sequences), while others may not.

**Line 3:**

Spacer.


**Line 4:**

Illumina quality score: http://www.illumina.com/documents/products/technotes/technote_understanding_quality_scores.pdf

This information may be used by your assembly software to filter out low quality sequences.

**An example of real Illumina paired-end data:**

\@HWI-D00289:185:C6FE5ANXX:8:1101:1178:2176 1:N:0:ACTTGA

AACTACATGTTCTTCAAATATACTTAGTTGGTCTTTCCAACATTCTCCAA

\+

\<\/\<\/\<\<FFFFFFFFFFFFFBF\<FFFFFFFF\/F\<FFFFFFFFFF\<FFFFFF\<

# Illumina Data Processing

## Quality control

You should always check to make sure the data you receive from the sequencing facility meets your expectations. Did you get the number of reads you expected, and are the reads the right length? Are the unique sample adapter barcodes you used well-represented in the library? How does quality of base calls vary along the length of the read?

You can use FastQC to get a basic summary of quality from your sequences, and there are a few bash one-liners that will help you look at barcode sequences. *If you do not know what that means, fear not!* That's something to learn once you have your data in hand, and is beyond the scope of this course. Your sequencing facility should be able to offer guidance about QC best practices for the type of data they deliver.

## Demultiplexing

Raw NGS data files are very large and contain the sequences for all individuals in your sample mixed together. Individuals are separated by sequence of the barcodes ligated to DNA during the library prep.

Sometimes demultiplexing software also looks to make sure the expected enzyme cut sites are present and removes reads missing this sequence. Software may also filter out low-quality sequences.

## Reference-based assembly

If your organism of interest has a pre-assembled whole genome sequence, you can use it as a reference to help assemble your short read data.

## *de novo* Assembly

## Quality Filters
**genotype quality**

- guideline: remove SNPs with quality scores <25

- rationale: remove low quality genotypes from analysis
  
**depth of coverage**

- guideline: retain sequences only with >=10X

- rationale: include only high-quality genotypes observed several times in the same individal for increased confidence in genotype calls
  
**minor allele frequency**

- guideline: remove <5% frequency

- rationale: remove private alleles and sequencing errors not caught in other filters, at the risk of also removing informative information about rare alleles

\pagebreak

# Overview of data formats and maniplation of VCF data in R

One of the most challenging aspects of analyzing NGS data is converting data to the different formats required for input into analysis software.

Here is a brief summary of the different data types types used by common R packages for population genetic inference.

|Package|Input Format|Link|
|:-------:|:------------:|:-------------------|
|adegenet|genlight |http://adegenet.r-forge.r-project.org/|
|genetics|genotype file|https://cran.r-project.org/web/packages/genetics/index.html|
|SNPrelate|GDS|http://www.bioconductor.org/packages/release/bioc/html/SNPRelate.html|
|poppr|geneclone|http://grunwaldlab.cgrb.oregonstate.edu/poppr-r-package-population-genetics|
|GWAStools|NetCDF, GDS or Matrix|http://www.bioconductor.org/packages/release/bioc/html/GWASTools.html|

For a more detailed discussion of different data types and their pros/cons, please see https://github.com/NESCent/r-popgen-hackathon/wiki/R-Classes-for-Population-Genetic-Data .

\pagebreak



### _Rough outline for option 1_
<!---working through the tutorial found here: https://cran.r-project.org/web/packages/PopGenome/vignettes/An_introduction_to_the_PopGenome_package.pdf 

Actually, it appears that PopGenome is calling the tick sample names chromosomes, and this is
confusing the loading. Don't use PopGenome!
-->

```{r, echo=FALSE, eval=FALSE}
# for Windows/OSX
install.packages("PopGenome")
# for Ubuntu
install.packages("~/Downloads/PopGenome_2.1.6.tar.gz", repos=NULL, type="source")
install.packages("~/Downloads/WhopGenome_0.9.2.tar.gz", repos=NULL, type="source")

# read in the VCF file as a regular table to get some quick info...
tick.table<-read.table('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED.vcf', sep="\t", header=T, comment.char="")


# make it appear as though all those snps are on the same chromosome
tick.table[,1]<-rep('1', length(tick.table[,1]))

# fix the names
names(tick.table)[1]<-"#CHROM"

write.table(tick.table, '~/Desktop/CSU_PopGen_Labs/Data/chrom1_tick_dummy_data.vcf', quote=FALSE, row.names=FALSE, sep="\t")

min(tick.subset$POS)
max(tick.table$POS)

# tick dummy data isn't great... what about human data from NCBI?

human.table<-read.table('~/Downloads//00-common_all.vcf.gz')

# utility just for reading VCF files into R
source("http://bioconductor.org/biocLite.R")
biocLite("VariantAnnotation")
biocLite("snpStats")
library(VariantAnnotation)

setwd('~/Desktop/CSU_PopGen_Labs/')

# dataset with indels
fl<-system.file('extdata', 'chr7-sub.vcf.gz', package='VariantAnnotation')
vcf1<-readVcf(fl, 'hg19')
vcf1
str(vcf1)

vcf.table<-read.table(fl)
dim(vcf.table)

vcf.table[1:10, 1:10]

test<-read.table('full_chrom1_tick_dummy_data.vcf.gz', header=T, comment.char = #)


vcf2<-readVcf('full_chrom1_tick_dummy_data.vcf.gz', character('1'))
vcf2
str(vcf2)
info(vcf2)
info(header(vcf2))

# find out what data are reported in the "FORMAT" column of the VCF file
geno(vcf2)

# get the definitions for those codes
geno(header(vcf2))

# return the sample names
samples(header(vcf2))

qual(vcf2)
min(qual(vcf2))
max(qual(vcf2))
hist(qual(vcf2))


ref(vcf2)
alt(vcf2)

info(vcf2)



# are all the data SNPs with one alt. allele?
isSNP<-isSNV(vcf2)
length(isSNP[which(isSNP==F)])

# if all are SNPs with one alt. allele, we can easily count transitions and transversions

isTransit<-isTransition(vcf2)
n.Transit<-length(isTransit[which(isTransit==T)]); n.Transit

# there is no fxn for detecting transversions, but since we know there is only one alternate allele for each SNP, if it isn't a transition it must be a transversion

n.Transver<-length(isTransit[which(isTransit==F)]); n.Transver

pie(c(n.Transit, n.Transver), labels=c('Transition', 'Transversion'), col=c('white', 'gray'))
# note different class for allele.2 -- there can be more than one alternate allele





snpmat<-genotypeToSnpMatrix(vcf1)

library(ade4)
data(humDNAm)
attributes(humDNAm)
amovahum<-amova(humDNAm$samples)
amovahum

#http://grunwaldlab.github.io/Population_Genetics_in_R/AMOVA.html
```

<!--- some command line things to index the newly created VCF file

# retrieve the header for the VCF file... opening it partially in gedit allows us to scroll down and see the first 417,187 lines are header. put those in a new file. (we're using data I processed through my merge_vcf.r script, which removes the header)
$ cd ~/Desktop/D_variabilis_Pseudoref/MasterPseudoRefVCF_Copy/
$ head -417187 pseudoref_mapped_genotypes.vcf >> pseudoref_vcf_header.txt
$ cp pseudoref_vcf_header.txt ~/Desktop/CSU_PopGen_Labs/

# paste the header onto the filtered data file we're using
$ cd ~/Desktop/CSU_PopGen_Labs/
$ cat pseudoref_vcf_header.txt chrom1_tick_dummy_data.vcf >> full_chrom1_tick_dummy_data.vcf

# index the full VCF file
$ sudo apt-get install tabix
$ bgzip full_chrom1_tick_dummy_data.vcf
$ tabix -f full_chrom1_tick_dummy_data.vcf.gz
-->



# References
>>>>>>> Stashed changes
