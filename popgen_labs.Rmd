---
title: "Potential Population Genetics Labs & Content"
output: pdf_document
bibliography: /home/antolinlab/Desktop/library.bib
---

<!--- bibliography: /users/kelly/desktop/library.bib -->

<!--- Set text wrapping for embedded R code -->
```{r set-options, echo=FALSE, }
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60))
```

<!--- Most of the old stuff was a bit beyond the scope of the course, so I've streamlined it a bit -->
# Next-generation sequencing (NGS)

Next-generation sequencing commonly refers to a set of DNA sequencing platforms that perform *short-read* sequencing. Short-read sequencing refers to the limit on the number of bases that can be read; typically less than 250 bases can be read in a single contiguous sequence. Users specify the exact number of bases they would like read, in increments of 25 bases, up to the 250 base limit (e.g., 50, 75, 100 bases, etc.) Traditional sequencing methods can sequence much longer contiguous stretches of DNA. The benefit of next-generation sequencing is that these platforms are *high-throughput*: they allow many samples to be sequenced in a *multiplexed* fashion where DNA from many individuals can be uniquely *barcoded* and mixed together. This produces high volumes of sequence data. NGS platforms can be used to sequence full genomes, or to sequence smaller subsets of the genome at a lower cost. Our lab exercises will focus on *reduced-representation* NGS, where genomic DNA is broken up either by restriction enzymes or sonication, and a small subset of the resulting fragments are sequenced.

The increased efficiency of NGS comes at a cost. In addition to the short-read constraint, NGS is a very error-prone technology. The expected probability that a nucleotide base will be incorrectly read is 0.1% -- while this is a small number, when hundreds of millions of bases are read, the result is a number of mis-identified bases. This measurement error is compensated for by re-reading each piece of DNA multiple times. The number of times the same piece of DNA is sequenced is called the *depth of coverage* for that sequence. Each base that is read by the sequencing instrument is also assigned a quality score that helps in determining the true sequence.

Inferring true sequence identity from the sequence data delivered by the sequencing instrument is by itself a rich area of research. Bioinformaticians develop algorithms to handle the massive quantities of data generated by NGS projects, identify sequencing errors using depth of coverage and quality scores, and ultimately determine the sequence of DNA to a high degree of statistical certainty. This work typically requires a supercomputer, but small datasets can be handled by personal computers. You will be working with data that has already been processed through the appropriate bioinformatic *pipeline* to remove sequence errors and *call genotypes*. 

Processing NGS data through these bioinformatic pipelines is beyond the scope of this course, but it is worth understanding (briefly) what it takes to go from a DNA sample to the sequence data needed to answer population-genetic questions.

## 1. Demultiplexing & quality filtering raw data

- Check that the length of each read matches the expectation.
- If DNA was prepared for sequencing using restriction digestion, check that the enzyme overhangs expected are present in the sequence.
- Separate sequences from different individuals using their unique sample barcodes.

## 2. Assembly
### 2a. Reference-based assembly

    - If the samples you have sequenced are from an organism with a fully sequenced genome, such as humans or yeast, then the reference genome sequence is very useful in determining which new DNA sequences are correct (error-free) and from which part of the genome they were sequenced. 

# Glossary
|Term|Definiton|
|:-------|:------------|
|next-generation sequencing (NGS)||
|short-read||
|high-throughput||
|multiplexed||
|sequence-by-synthesis||
|depth of coverage||
|pipeline||
|genotype calling||
|quality score||
|barcode||

Brief overview and list of platforms.
Some discussion of error rates & necessary coverage depths.

## Uses & Common Techniques

- Whole genome sequencing & re-sequencing
- Biodiversity studies (e.g., gut microbiome or soil microbial diversity)
- Reduced representation genome sequencing
    - RADseq
    - 2b-RAD
    - ddRADseq

# NGS Data Processing

This will be an overview of the general workflow for handling NGS data derived from reduced-representation and whole genome resequencing projects. Links to online resources and tutorials will be included.

## Demultiplexing

Raw NGS data files are very large and contain the sequences for all individuals in your sample mixed together. The unique sample adapters ligated to the restriction fragments to sequenc

## Reference-based assembly

## *de novo* Assembly

## Quality Filters
**genotype quality**

- guideline: remove SNPs with quality scores <25

- rationale: remove low quality genotypes from analysis
  
**depth of coverage**

- guideline: retain sequences only with >=10X

- rationale: include only high-quality genotypes observed several times in the same individal for increased confidence in genotype calls
  
**minor allele frequency**

- guideline: remove <5% frequency

- rationale: remove private alleles and sequencing errors not caught in other filters, at the risk of also removing informative information about rare alleles

\pagebreak

# Overview of data formats and maniplation of VCF data in R

One of the most challenging aspects of analyzing NGS data is converting data to the different formats required for input into analysis software.

Here is a brief summary of the different data types types used by common R packages for population genetic inference.

|Package|Input Format|Link|
|:-------:|:------------:|:-------------------|
|adegenet|genlight |http://adegenet.r-forge.r-project.org/|
|genetics|genotype file|https://cran.r-project.org/web/packages/genetics/index.html|
|SNPrelate|GDS|http://www.bioconductor.org/packages/release/bioc/html/SNPRelate.html|
|poppr|geneclone|http://grunwaldlab.cgrb.oregonstate.edu/poppr-r-package-population-genetics|
|GWAStools|NetCDF, GDS or Matrix|http://www.bioconductor.org/packages/release/bioc/html/GWASTools.html|

For a more detailed discussion of different data types and their pros/cons, please see https://github.com/NESCent/r-popgen-hackathon/wiki/R-Classes-for-Population-Genetic-Data .

\pagebreak

# Exercise 1: Structure at the locus-level: Calculation of $F_{ST}$

## Background

Genetic variation in populations can exist within subpopulations ("groups") and between groups. We often ask how divergent subpopulations are by considering the partitioning of genetic variation into these categories. If most of the population genetic variation is partitioned **between** groups, we infer that these groups are genetically diverged in some manner. We would then evaluate possible hypotheses for this divergence. 

Alternatively, if most of the population genetic variation is partitioned **within** groups, we infer that the groups are not highly diverged. In this case, even if there is a high amount of genetic variation in the poplation as a whole, there is no difference in the genetic variation contained within each group.

We can measure genetic variation in subpopulations in a number of ways. The classical method is to calculate the Fixation Index, $F_{ST}$ [@Brown1970]. High $F_{ST}$ values indicate genetic variation is partitioned **between** groups and suggests the presence of meaningful population structure.

Evolutionary rates vary throughout the genome, and consequently $F_{ST}$ can vary for different loci under consideration. Just as some loci are highly conserved while others are highly divergent, $F_{ST}$ can be very high for some loci and very low for others. Next-generation sequencing produces data for numerous loci, and we can calculate $F_{ST}$ for each locus. This provides us with a distribution of $F_{ST}$ values across the genome a nd allows us to identify sites that correspond with population structure.

### Exercise options

1. Find a reference mapped dataset and calculate sliding window $F_{ST}$
2. Take a small subset of the tick dataset and do an $F_{ST}$ outlier analysis using Bayescan (would need to be installed in computer lab or on a remote machine to which students can submit computing jobs).
3. Take already completed Bayescan output, visualize, and interpret


### _Rough outline for option 1_
<!---working through the tutorial found here: https://cran.r-project.org/web/packages/PopGenome/vignettes/An_introduction_to_the_PopGenome_package.pdf -->

```{r, echo=FALSE, eval=FALSE}
# for Windows/OSX
install.packages("PopGenome")
# for Ubuntu
install.packages("~/Downloads/PopGenome_2.1.6.tar.gz", repos=NULL, type="source")
install.packages("~/Downloads/WhopGenome_0.9.2.tar.gz", repos=NULL, type="source")

# read in the VCF file as a regular table to get some quick info...
tick.table<-read.table('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED.vcf', sep="\t", header=T, comment.char="")

# column 2 is the position
min(tick.table[,2])
max(tick.table[,2])

# pretend we have a single chromosome represented in this VCF file... how many bases would there be?
200*length(tick.table[,1])
# 477,400 is a bit long for a chromosome

# make a fake 100,000 base chrom
tick.subset<-tick.table[1:(100000/200),]

# update the position info to make all the position points unique
# each row represents a SNP found on a 200bp long concatenated segment of the reference
# some 200bp stretches have >1 SNP, but since we're making stuff up anyway we won't really worry about that
for(row in 1:length(tick.table[,1])){
  tick.table[row,2]=tick.table[row,2]+(row*200)
}

pop1<-names(tick.table)[10:92]
pop1.fix<-sub('.', '-', pop1, fixed=T)
pop2<-names(tick.table)[93:108]
pop2.fix<-sub('.', '-', pop2, fixed=T)

write.table(pop1.fix, file='~/Desktop/CSU_PopGen_Labs/pop1.txt', quote=F, row.names=F, col.names=F)
write.table(pop2.fix, file='~/Desktop/CSU_PopGen_Labs/pop2.txt', quote=F, row.names=F, col.names=F)

head(tick.subset)[,1:5]
tail(tick.subset)[,1:5]

# make it appear as though all those snps are on the same chromosome
tick.table[,1]<-rep('1', length(tick.table[,1]))

# fix the names
names(tick.table)[1]<-"#CHROM"

write.table(tick.table, '~/Desktop/CSU_PopGen_Labs/chrom1_tick_dummy_data.vcf', quote=FALSE, row.names=FALSE, sep="\t")

min(tick.subset$POS)
max(tick.table$POS)

# tick dummy data isn't great... what about human data from NCBI?

human.table<-read.table('~/Downloads//00-common_all.vcf.gz')

# utility just for reading VCF files into R
source("http://bioconductor.org/biocLite.R")
biocLite("VariantAnnotation")
biocLite("snpStats")
library(VariantAnnotation)
fl<-system.file('extdata', 'chr7-sub.vcf.gz', package='VariantAnnotation')
vcf1<-readVcf(fl, 'hg19')
vcf1

snpmat<-genotypeToSnpMatrix(vcf1)

library(ade4)
data(humDNAm)
attributes(humDNAm)
amovahum<-amova(humDNAm$samples)
amovahum

#http://grunwaldlab.github.io/Population_Genetics_in_R/AMOVA.html
```

<!--- some command line things to index the newly created VCF file

# retrieve the header for the VCF file... opening it partially in gedit allows us to scroll down and see the first 417,187 lines are header. put those in a new file. (we're using data I processed through my merge_vcf.r script, which removes the header)
$ cd ~/Desktop/D_variabilis_Pseudoref/MasterPseudoRefVCF_Copy/
$ head -417187 pseudoref_mapped_genotypes.vcf >> pseudoref_vcf_header.txt
$ cp pseudoref_vcf_header.txt ~/Desktop/CSU_PopGen_Labs/

# paste the header onto the filtered data file we're using
$ cd ~/Desktop/CSU_PopGen_Labs/
$ cat pseudoref_vcf_header.txt chrom1_tick_dummy_data.vcf >> full_chrom1_tick_dummy_data.vcf

# index the full VCF file
$ sudo apt-get install tabix
$ bgzip full_chrom1_tick_dummy_data.vcf
$ tabix -f full_chrom1_tick_dummy_data.vcf.gz
-->

```{r, echo=FALSE, results='hide', include=FALSE}
library(PopGenome)
#library(WhopGenome)
```

```{r, eval=FALSE, tidy=TRUE}
library(PopGenome)
#library(WhopGenome)
```

```{r, eval=FALSE}
# PopGenome is one of the few R packages that can read VCF files directly.
# We'll use the pseudoreference mapped tick genome data
# to calculate a sliding window FST -- totally inappropriate because this is just a subset of the tick genome...
# But it's a chance to learn how the software works.

# population membership
setwd('~/Desktop/CSU_PopGen_Labs/')

# explore the raw data in R (but we can't do analysis directly on this file)
tick.table<-read.table('chrom1_tick_dummy_data.vcf', sep="\t", header=T, comment.char="")

# get population IDs
pop1<-names(tick.table)[10:92]
pop1.fix<-sub('.', '-', pop1, fixed=T)
pop2<-names(tick.table)[93:108]

populations<-list(pop1, pop2)

GENOME.class<-readVCF('full_chrom1_tick_dummy_data.vcf.gz', numcols=2, tid="1", frompos=338, topos=200044)

GENOME.class@n.sites
GENOME.class@n.biallelic.sites #says there are only 13?! something wrong with numcols argument, I think...

GENOME.class<-set.populations(GENOME.class, populations)

GENOME.class.slide<-sliding.window.transform(GENOME.class,100,100)

GENOME.class.slide<-diversity.stats(GENOME.class.slide)
PopGplot(GENOME.class.slide@nuc.diversity.within)

slide.GENOME.slide<-F_ST.stats(GENOME.class, list(1:83, 84:99))
get.F_ST(GENOME.class.slide)
fst.vals<-GENOME.class.slide@F_ST.stats
GENOME.class@Pi
GENOME.class@region.stats

# none of this works right...

# try with stickleback data...
# vcf file from http://datadryad.org/resource/doi:10.5061/dryad.62hb0/1
# lines are out of order; fix with 
# $ vcf-sort batch_2.vcf.gz >> batch_2_sort.vcf
# then deal with the unfortunate reality that this vcf file has missing GT data as ".",
# and not "./." -- this means that PopGenome can't parse the data
# run a short python script that will fix the problem (thx Joe!)... FILE NAMES ARE HARD-CODED
# $ python GTfix.py
# this inserts new lines after every line; rawr! fix is with this:
# $ grep . batch_2_sort_GTfix.vcf >> batch_2_sort_GTrefix.vcf
# then zip with bgzip and index with tabix (see above html-style comment for the basic structure)

stick<-read.table('batch_2_sort.vcf.gz')
stick.fix<-read.table('batch_2_sort_GTfix.vcf.gz', sep="\t")

groupX<-stick.fix[which(stick.fix[,1]=='groupX'),]

setwd('~/Downloads/or_stickleback/')
setwd('~/Desktop/CSU_PopGen_Labs/')
GENOME.class<-readVCF('batch_2_sort_GTrefix.vcf.gz', tid='groupX', numcols=length(stick.fix[,1]), frompos=min(groupX[,2]), topos=max(groupX[,2]))

GENOME.class<-F_ST.stats(GENOME.class, list(1:250, 251:500))
get.F_ST(GENOME.class)

GENOME.class@n.sites
GENOME.class@n.biallelic.sites 

GENOME.class<-set.populations(GENOME.class, populations)

GENOME.class.slide<-sliding.window.transform(GENOME.class)

GENOME.class.slide<-diversity.stats(GENOME.class.slide)
PopGplot(GENOME.class.slide@nuc.diversity.within)
```

### _Rough outline for option 3_

## Objective

Identify $F_{ST}$ outliers from a large SNP dataset and interpret results.

## Required Software

```{r}
library(qvalue)
```

## Dataset

<!--- the bayescan output loaded below ignores population structure by site and looks for outliers across both populations simultaneously
(these are the data from the EEID 2014 poster & from my dissertation)
-->

```{r}
# load in the Bayescan data
bayescan<-read.table('~/Dropbox/ddRADseq/D_variabilis_Pseudoref/Bayescan_Input_pseudoref_pop_filtered_maxmissing0.75_opossums_NotMis_fst.txt')
```

We have `r length(bayescan[,1])` quality-filtered SNPs from the tick *Dermacentor variabilis*. The SNPs were obtained from a ddRADseq project and stored in a VCF file. VCF data for the SNPs was reformatted for analysis in **Bayescan** [@Foll2008]. **Bayescan** uses SNP allele counts to calculate $F_{ST}$ but requires a supercomputer to run. Consquently, you are provided with the output from a Bayescan analysis.

## Assignment

### The Bayescan Model

Bayescan uses a model where SNP allele frequencies are described by parameters for *locus-level* and *population-level* effects. These two parameters are estimated for each SNP in the dataset. Our null hypothesis is that genetic drift is the primary determinant of allele frequencies. When this is the case, the locus-level parameter is 0. If the locus-level parameter for a SNP is significantly different than 0 we have evidence consistent with natural selection acting on allele frequencies. Non-zero locus-level parameters correspond with higher $F_{ST}$ values. Bayescan reports both the calculated $F_{ST}$ and a p-value indicating whether the corresponding locus-level parameter was significantly different from the null expectation of genetic drift. **You are provided with the Bayescan $F_{ST}$ and p-value data.**



### Multiple Comparisions in Genetic Inference

NGS studies, including reduced-representation genome sequencing studies, can identify 1,000s of SNPs. When we calculate $F_{ST}$ or any other descriptive statistic for each SNP, we also want to determine if those values are *statistically significant*; that is, do they differ from our null hypothesis? Recall from basic statistics that we infer significance when our p-values are less than our set $\alpha$ level. The $\alpha$ level is the maximum probability of Type I (false-positive) error; typically $\alpha$ = 0.05. 

If we perform `r length(bayescan[,1])` statistical tests -- one for each SNP -- and expect that 5% of our p-values would be statistically significant by chance alone, how many false-positive results would we obtain? What might be the consequence of incorrectly inferring that SNPs have significantly high $F_{ST}$ values when they really do not?

We can correct our p-values to account for the number of comparisions we perform. One typical technique for genetic datasets is to calculate a *qvalue*. The *qvalue* for a SNP will always be higher than the p-value, but if the allele frequencies for the SNP are truly significantly different from our null model of drift then the *qvalue* will still be less than $\alpha$.

#### How many SNPs have significant $F_{ST}$ at the $\alpha$ = 0.05 level?
Shown below is an example for counting SNPs at the $\alpha$ = 0.1 level; modify the code below to determine which SNPs have allele frequencies consistent with selection at the more stringent $\alpha$ = 0.05 level.

```{r}
# count the number significant
sig<-bayescan$qval[bayescan$qval < 0.1]
length(sig)

# Set plot margins
par(mar=c(5,6,4,2))

# Add axis labels and an appropriate title to your figure
plot(bayescan$qval, bayescan$fst, pch=16, cex=1.5, cex.axis=1.5, xlim=c(0,1), 
     ylim=c(0,0.15), bty='n', las=1, xlab='', ylab='', cex.lab=1.5)

# All the high q-value points are on top of each other; use a histogram to look at the distribution of q-values\
# Add axis labels and an appropriate title to your figure
hist(bayescan$qval, breaks=seq(0,1,0.05), xlab='', main='')
# Draw a vertical line to indicate your alpha level
abline(v=0.1, col='red')
```

\pagebreak

#Exercise 2: Structure at the genome level

## Background

However, in order to use $F_{ST}$ we need to have a hypothesis about what our subpopulations are. For example, we could calculate $F_{ST}$ for animals collected from subpopulations at different locations or from distinct habitat types. However, in many cases we don't have enough information to make good hypotheses regarding the number

## Objective 

Look for population structure that corresponds with tick collection site.

## Required Software

```{r eval=FALSE, tidy=TRUE}
# for OSX and Windows users, download directly from CRAN
install.packages('adegenet')
# for Ubuntu users, download source code and install manually
install.packages("~/your/path/adegenet", repos=NULL, type="source")

# packages
library(scales)
library(adegenet)
```

## Dataset

Brief description.

<!--- Execute the R code but don't show it because it prints too many messages to the output -->
### Loading genlight data
```{r echo=FALSE, results='hide', include=FALSE}
library(scales)
library(adegenet)

# read in data
site<-read.snp('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED_FOR_ADEGENET.finalSNPs_site.snp')
extra.data<-read.csv('~/Desktop/UT_ddRADseq/ddRAD_FinalLibrary_SampleInfo_Full.csv')
```

<!--- Show the R code but don't execute it -->

```{r eval=FALSE, tidy=TRUE}
# read in data
site<-read.snp('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED_FOR_ADEGENET.finalSNPs_site.snp')
extra.data<-read.csv('~/Desktop/UT_ddRADseq/ddRAD_FinalLibrary_SampleInfo_Full.csv')
```

## Assignment

Brief background on discriminant analysis, principle components, and discriminant analysis of principle components (DAPC) [@Jombart2010]

**Finding population structure without an *a priori* hypothesis for population membership.**
You may be unsure about how many subpopulations exist in your population. In this case it is possible to estimate the number of subpopulations. **adegenet** can use likelihood-based methods to select the number of subpopulations most consistent with the genetic data at hand.


```{r, eval=FALSE}
# Adegenet will ask you to determine the number of clusters to return based on the BIC value
guess.k<-find.clusters(site, n.pca=33)

```

```{r, echo=FALSE}
fake.bic<-c(400, 250, 125, 100, 105, 107, 108, 110, 111, 115)
```

You will be prompted to select a number of clusters to retain -- choose something high like 10. You will then see a graph showing BIC value versus number of clusters. The number of clusters with the lowest BIC value is the one most highly supported by the data. 

Here is an example figure of **totally made up data** -- your BIC curve may look different. In this example, the lowest BIC corresponds to `r min(fake.bic)` clusters.

```{r, echo=FALSE}
plot(seq(1,10,1), fake.bic, col='blue', xlab='Number of clusters', ylab='BIC', main='Value of BIC\nversus number of clusters')
lines(seq(1,10,1), fake.bic, col='blue')
```

What is the lowest BIC for the tick dataset? Does this support the hypothesis that the population is structured?

**Testing for structure in hypothesized populations.**
When you know what your subpopulations the individuals in your study should belong to, you can test specifically for that structure. You provide the DAPC function information on population membership for each individual, and DAPC identifies the genotypes that best differentiate your populations.
```{r}
# discriminant analysis of principal components to detect clusters
site.dapc<-dapc(site, pop=site@pop, n.pca=33, n.da=1) # save as many PCs as possible
par(mar=c(5,5,2,2))
site.colors2<-c('#fec44f', '#fee391')
scatter(site.dapc, col=c('#fec44f', '#fee391'), axes=FALSE)
legend("topright", legend=c('HB', 'SRT'), fill=alpha(site.colors2, 0.5), bty='n')
```

**Testing the predictive power of DAPC with novel genotypes.**
Because you tell DAPC exactly which population each indiviual comes from, it's not terribly surprising that you recover evidence in support of your hypothesis. How well do you think DAPC would perform if you gave the method a novel genotype and asked from which population that genotype was derived? If you truly have strong population structure, novel genotypes should be correctly assigned to the appropriate subpopulaton.

Poor model fits will not produce consistent assignment of genotypes despite showing strong evidence of clustering when the full dataset is considered. Do you believe that tick populations are geographically structured? Why or why not? Are there any additional data you'd like to have to help answer this question?

<!--- Write a function students can source and use to do the n-1 predictive sampling. -->

```{r, eval=FALSE}
# load custom R script for n-1 predictive sampling
source n_minus_1.r 

n_minus_1(data)
```

<!--- note: cacheing (sp?) only works when you haven't modified the code! do any plotting in a separate code chunk to avoid redoing the computation -->
```{r cache=TRUE, echo=FALSE, results='hide'}
site.host.names<-read.table('~/Dropbox/ddRADseq/Final_Analysis/Structure_by_Site_Host/Final_Pseudoref_minmeanDP20_minGQ25_maf0.05_BOTH_SITES_host_filtered_only_maxmissing0.75_MERGED_FOR_ADEGENET.finalSNPs_site_and_host.snp', skip=6, sep="\n")
tick.names<-as.character(site.host.names[seq(1, 198, 2),])
tick.names.formatted<-gsub('> ', '', tick.names, perl=TRUE)

tally<-0
assign.site<-c()
true.site<-c()
tf.site<-c()
for(i in 1:nInd(site)){
  x.rm.site<-site[i] # remove individual i
  x.kp.site<-site[-i] # keep all but individual i
  x.kp.dapc.site<-dapc(x.kp.site, n.pca=32, n.da=1)
  predict.x.rm.site<-predict.dapc(x.kp.dapc.site, newdata=x.rm.site)
  if(as.character(predict.x.rm.site$assign)==as.character(pop(x.rm.site))){
    tally=tally+1
    tf.site<-c(tf.site, 1)
  }
  else{
    tf.site<-c(tf.site, 0)
  }
  print(as.character(predict.x.rm.site$assign))
  assign.site<-c(assign.site, as.character(predict.x.rm.site$assign))
  true.site<-c(true.site, as.character(pop(x.rm.site)))
  #print(tally)
}

what.worked.site<-cbind(tick.names.formatted, assign.site, true.site, tf.site) #names vector is pulled from the snp data, so the order should be preserved

hb<-length(subset(what.worked.site, true.site=='HB')[,1])
srt<-length(subset(what.worked.site, true.site=='SRT')[,1])
hb.correct<-length(subset(what.worked.site, true.site=='HB' & tf.site=='1')[,1])
srt.correct<-length(subset(what.worked.site, true.site=='SRT' & tf.site=='1')[,1])
hb.pc<-hb.correct/hb
srt.pc<-srt.correct/srt

accuracy.site<-(hb.correct+srt.correct)/length(what.worked.site[,1])
print(accuracy.site)
```

What fraction if individuals does DAPC accurately assign to the correct subpopulation? Is this consistent with the results from the full model? Do you believe tick populations are structured geographically?


```{r}
round(accuracy.site, digits=2)
```

\pagebreak
<!---
# Evaluating evolutionary hypotheses for observed population structure

Structure can arise under a number of processes. Differentiating genetic drift from natural selection is often the first objective once population structure has been identified.

# Controlling for relatedness of individuals

Related individuals share genetic information and the inclusion of many related individuals can sometimes confound questions about population structure.
-->

# References
